{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RCgvZIF45nS5"
      },
      "source": [
        "# Detect Pneumonia with Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TbPH277Agrn"
      },
      "source": [
        "## **Imports and data access**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "9f5aROq7317W",
        "outputId": "77d183fa-1b02-4f7b-a21e-363e2611cb9c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np \n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation, Concatenate, \\\n",
        "                                    AveragePooling2D, Input, Flatten, MaxPool2D, Dropout, Add\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.losses import  categorical_crossentropy\n",
        "import pandas as pd\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EZyzcS_5FgT",
        "outputId": "d95e1a11-6808-4fdb-bdec-095ab749759a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Connect to the Google Driver\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZIbTheM5GM-"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/Pneumonia_data/train_images/' #directory of the train images\n",
        "test_path = '/content/drive/MyDrive/Pneumonia_data/test_images/'  #directory of the test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MlxCz7M5Mke",
        "outputId": "7f53bc45-9d0f-4a00-9c75-837d808131e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4672 4672\n"
          ]
        }
      ],
      "source": [
        "df_labels = pd.read_csv('/content/drive/MyDrive/Pneumonia_data/labels_train.csv') #Create a data frame includes the Labels of the train dataset\n",
        "train_image = df_labels['file_name'].values #Getting the title of each label\n",
        "labels = df_labels['class_id'].values # Getting the label of each class_id \n",
        "print(len(train_image),len(labels)) # Just to be sure about the shape\n",
        "test_images_names = [img for img in os.listdir(test_path)] # Getting the images names from test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D5a9ClZ6dDa"
      },
      "source": [
        "## **Data pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS2jOtEelQf7"
      },
      "outputs": [],
      "source": [
        "#Creating a fuction that converts an image number into the file path where the image is located, opens the image, and returns the image as a numpy array.\n",
        "\n",
        "img_h = 256\n",
        "image_w = 256\n",
        "def get_image(row_ids, root='/content/drive/MyDrive/Pneumonia_data/train_images/'):\n",
        "    X = []\n",
        "    for image_id in row_ids:\n",
        "      file_path = os.path.join(root, image_id)\n",
        "      img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
        "      resized_X = cv2.resize(img, (img_h,image_w))\n",
        "      \n",
        "      X.append(resized_X)\n",
        "    return np.array(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrFSV-lLnpaF"
      },
      "outputs": [],
      "source": [
        "#Creating our X_train with the fuction get_image as numpy array\n",
        "X_train = get_image(train_image) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPUhRPfq4eU8",
        "outputId": "abc015d9-19cf-4efc-9cf6-90226f8b5452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4672, 256, 256)\n"
          ]
        }
      ],
      "source": [
        "# Just to be sure about the shape of our numpy array\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofQe0ud44eZs"
      },
      "outputs": [],
      "source": [
        "#Creating our X_test with the fuction get_image as numpy array\n",
        "X_test = get_image(test_images_names, root = test_path )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kndcVa8MpfPR",
        "outputId": "4d50496f-4986-4318-f628-2a5df3701aea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1168, 256, 256)\n"
          ]
        }
      ],
      "source": [
        "# Just to be sure about the shape of our numpy array\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBYy6KXF4eXz"
      },
      "outputs": [],
      "source": [
        "#Converting our labels to numpy array\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9vPJ9VtYWMQ",
        "outputId": "90e0d2a9-18f2-4de4-fb65-103bb56db89c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4672,)\n"
          ]
        }
      ],
      "source": [
        "# Just to be sure about the shape of our numpy array\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWAOYkJ89Bjl"
      },
      "outputs": [],
      "source": [
        "# Splitting our data to X_train, X_val, Y_train and Y_val. Setting the size of test set to 10% of the dataset and random_state equal to 1\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, labels, test_size=0.10, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3xuX74u4rkb"
      },
      "outputs": [],
      "source": [
        "#Just to be sure about the image size to 256, while we reshape the arrays and setting the input shape.\n",
        "#Also, we normalize the train, validation and test data.\n",
        "img_size = 256\n",
        "# reshape data for deep learning \n",
        "X_train = X_train.reshape(-1, img_size, img_size, 1)\n",
        "Y_train = np.array(Y_train)\n",
        "\n",
        "X_val = X_val.reshape(-1, img_size, img_size, 1)\n",
        "Y_val = np.array(Y_val)\n",
        "\n",
        "X_test = X_test.reshape(-1, img_size, img_size, 1)\n",
        "\n",
        "# Input image dimensions\n",
        "input_shape = X_train.shape[1:]\n",
        "\n",
        "# Normalize Train and Validation data\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_val = X_val.astype('float32') / 255.0\n",
        "\n",
        "X_train_mean = np.mean(X_train, axis=0)\n",
        "\n",
        "X_train -= X_train_mean\n",
        "X_val -= X_train_mean\n",
        "\n",
        "# Normalize Test data\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "X_test -= X_train_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RBByiUwGsir",
        "outputId": "b792e2a3-4535-46a1-8766-3b1c5dac0db8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4204, 256, 256, 1)\n",
            "(468, 256, 256, 1)\n"
          ]
        }
      ],
      "source": [
        "# Just to be sure about the shape of our numpy array\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNy2fwmhAk9U"
      },
      "outputs": [],
      "source": [
        "# With the use of keras library we convert our data to categorical\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "Y_train = to_categorical(Y_train)\n",
        "Y_val = to_categorical(Y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-NZ8EJP6yOU"
      },
      "source": [
        "## **Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIol3C-LlMGY"
      },
      "outputs": [],
      "source": [
        "#Setting imageDataGenerator that applies any random transformations on each training image as it is passed to the model\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range = 10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip = True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "#Setting a fuction that change the lr value depending on epoch iterations. \n",
        "def lr_schedule(epoch):\n",
        "    lr = 1e-3\n",
        "    if epoch > 100:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 60:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 40:\n",
        "        lr *= 1e-1\n",
        "    return lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydXbr6JLv6P-",
        "outputId": "1aea8b50-158f-44da-c8d9-42706da6aee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 256, 256, 1)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 256, 256, 32)      320       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256, 256, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256, 256, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 128, 128, 32)      0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128, 128, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 128, 128, 32)      9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128, 128, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 128, 128, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 64, 64, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 64)          36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_2 (Average (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 1,193,827\n",
            "Trainable params: 1,192,163\n",
            "Non-trainable params: 1,664\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Setting the number of classes as many as we want to predict, 3 \n",
        "num_classes = 3\n",
        "#Creating a fuction of the model we will use to make our predictions using as parameter the input_shape\n",
        "def cnn_2(input_shape):\n",
        "    x_in = Input(shape=input_shape) #Setting the starting x_in with the shape like our input shape which we created before, to fit to the model. \n",
        "    \n",
        "    x = Conv2D(filters=32, kernel_size=(3,3),kernel_initializer=\"he_normal\",kernel_regularizer=l2(1e-4), padding =\"same\")(x_in) # Using 2D convolution layer , with 32 filters, each kernel size 3,3. Also we use the set the kernel_initializer to he_normal and the regulizer to l2(1e-4), while setting padding to same.\n",
        "    x = BatchNormalization()(x) # We use Batch Normalization for stabilizing the learning process\n",
        "    x = Activation('relu')(x) # Setting the Activasion to relu\n",
        "    x = MaxPool2D(pool_size= (2))(x) #Setting MaxPool2D, pool_size = 2 to reduce its dimensionality\n",
        "    x = Dropout(rate=0.5)(x) #Setting Dropout rate to 0.5 to reduce the capacity\n",
        "  \n",
        "    x = Conv2D(filters=32, kernel_size=(3,3),kernel_initializer=\"he_normal\",kernel_regularizer=l2(1e-4), padding =\"same\")(x) # Creating a new 2D convolution layer , with 32 filters, each kernel size 3,3. Also we use the set the kernel_initializer to he_normal and the regulizer to l2(1e-4), while setting padding to same.\n",
        "    x = BatchNormalization()(x)# We use Batch Normalization for stabilizing the learning process\n",
        "    x = Activation('relu')(x)# Setting the Activasion to relu\n",
        "    x = MaxPool2D(pool_size= (2))(x)#Setting MaxPool2D, pool_size = 2 to reduce its dimensionality\n",
        "    x = Dropout(rate=0.3)(x)#Setting Dropout rate to 0.3 to reduce the capacity\n",
        "\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3),kernel_initializer=\"he_normal\",kernel_regularizer=l2(1e-4), padding=\"same\")(x) # Creating a new 2D convolution layer , with 64 filters, each kernel size 3,3. Also we use the set the kernel_initializer to he_normal and the regulizer to l2(1e-4), while setting padding to same.\n",
        "    x = BatchNormalization()(x)# We use Batch Normalization for stabilizing the learning process\n",
        "    x = Activation('relu')(x)# Setting the Activasion to relu\n",
        "    x = MaxPool2D(pool_size= (2))(x)#Setting MaxPool2D, pool_size = 2 to reduce its dimensionality\n",
        "    x = Dropout(rate=0.2)(x) #Setting Dropout rate to 0.2 to reduce the capacity\n",
        "\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3),kernel_initializer=\"he_normal\",kernel_regularizer=l2(1e-4), padding=\"same\")(x) # Creating a new 2D convolution layer , with 64 filters, each kernel size 3,3. Also we use the set the kernel_initializer to he_normal and the regulizer to l2(1e-4), while setting padding to same.\n",
        "    x = BatchNormalization()(x)# We use Batch Normalization for stabilizing the learning process\n",
        "    x = Activation('relu')(x)# Setting the Activasion to relu\n",
        "    x = AveragePooling2D(pool_size= (2))(x)#Setting MaxPool2D, pool_size = 2 to reduce its dimensionality\n",
        "    x = Dropout(rate=0.1)(x) #Setting Dropout rate to 0.1 to reduce the capacity\n",
        "\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3),kernel_initializer=\"he_normal\",kernel_regularizer=l2(1e-4), padding=\"same\")(x) # Creating a new 2D convolution layer , with 64 filters, each kernel size 3,3. Also we use the set the kernel_initializer to he_normal and the regulizer to l2(1e-4), while setting padding to same.\n",
        "    x = BatchNormalization()(x)# We use Batch Normalization for stabilizing the learning process\n",
        "    x = Activation('relu')(x)# Setting the Activasion to relu\n",
        "    x = AveragePooling2D(pool_size= (2))(x)#Setting MaxPool2D, pool_size = 2 to reduce its dimensionality\n",
        "    x = Dropout(rate=0.05)(x) #Setting Dropout rate to 0.05 to reduce the capacity\n",
        "\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3),kernel_initializer=\"he_normal\",kernel_regularizer=l2(1e-4), padding=\"same\")(x) # Creating a new 2D convolution layer , with 64 filters, each kernel size 3,3. Also we use the set the kernel_initializer to he_normal and the regulizer to l2(1e-4), while setting padding to same.\n",
        "    x = BatchNormalization()(x)# We use Batch Normalization for stabilizing the learning process\n",
        "    x = Activation('relu')(x)# Setting the Activasion to relu\n",
        "    x = AveragePooling2D(pool_size= (2))(x)#Setting MaxPool2D, pool_size = 2 to reduce its dimensionality\n",
        "    x = Dropout(rate=0.05)(x) #Setting Dropout rate to 0.05 to reduce the capacity\n",
        "\n",
        "    x = Flatten()(x) #We flatten the output of the convolutional layers to create a single long feature vector\n",
        "    x = Dense(512, activation='relu')(x)# Setting Dense 512 the Activasion to relu\n",
        "    x = Dropout(rate=0.2)(x)#Setting Dropout rate to 0.2 to reduce the capacity\n",
        "    x = Dense(512, activation='relu')(x)# Setting Dense 512 the Activasion to relu\n",
        "    x = BatchNormalization()(x)# We use Batch Normalization for stabilizing the learning process\n",
        "    x = Dropout(rate=0.2)(x)#Setting Dropout rate to 0.2 to reduce the capacity\n",
        "    x = Dense(512, activation='relu')(x)# Setting Dense 512 the Activasion to relu\n",
        " \n",
        " # Complete our model with dense with num_classes = 3 and softmax activation, using optimizer Adam, and the lr_schedule we created before. Metric = Accuracy   \n",
        "    out = Dense(num_classes, activation='softmax')(x) \n",
        "    model = Model(inputs=x_in, outputs=out)\n",
        "    return model\n",
        "\n",
        "model = cnn_2(input_shape)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', #Using categorical_crossentropy\n",
        "              optimizer=Adam(lr=lr_schedule(0)), # Optimizer Adam, with lr_schedule\n",
        "              metrics=['accuracy']) #Using accurace as metric\n",
        "model.summary()   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEdKsKninsa5",
        "outputId": "b2abee30-7cc7-4f80-e814-a7a111e943f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "132/132 [==============================] - 55s 160ms/step - loss: 1.0903 - accuracy: 0.5664 - val_loss: 13.6634 - val_accuracy: 0.4722\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.47222, saving model to model.h5\n",
            "Epoch 2/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.8305 - accuracy: 0.6758 - val_loss: 9.8370 - val_accuracy: 0.2415\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.47222\n",
            "Epoch 3/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.7258 - accuracy: 0.7165 - val_loss: 8.5581 - val_accuracy: 0.4722\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.47222\n",
            "Epoch 4/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.6879 - accuracy: 0.7366 - val_loss: 3.6357 - val_accuracy: 0.4786\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.47222 to 0.47863, saving model to model.h5\n",
            "Epoch 5/120\n",
            "132/132 [==============================] - 20s 150ms/step - loss: 0.6418 - accuracy: 0.7555 - val_loss: 1.5698 - val_accuracy: 0.4701\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.47863\n",
            "Epoch 6/120\n",
            "132/132 [==============================] - 20s 150ms/step - loss: 0.6296 - accuracy: 0.7596 - val_loss: 1.0095 - val_accuracy: 0.6090\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.47863 to 0.60897, saving model to model.h5\n",
            "Epoch 7/120\n",
            "132/132 [==============================] - 20s 150ms/step - loss: 0.6213 - accuracy: 0.7646 - val_loss: 1.7316 - val_accuracy: 0.5855\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.60897\n",
            "Epoch 8/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.6397 - accuracy: 0.7608 - val_loss: 0.9267 - val_accuracy: 0.5940\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.60897\n",
            "Epoch 9/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.5908 - accuracy: 0.7823 - val_loss: 0.9140 - val_accuracy: 0.6560\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.60897 to 0.65598, saving model to model.h5\n",
            "Epoch 10/120\n",
            "132/132 [==============================] - 20s 154ms/step - loss: 0.6101 - accuracy: 0.7722 - val_loss: 0.8522 - val_accuracy: 0.7115\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.65598 to 0.71154, saving model to model.h5\n",
            "Epoch 11/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.6009 - accuracy: 0.7795 - val_loss: 0.8639 - val_accuracy: 0.6838\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.71154\n",
            "Epoch 12/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.5724 - accuracy: 0.7922 - val_loss: 1.0606 - val_accuracy: 0.6303\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.71154\n",
            "Epoch 13/120\n",
            "132/132 [==============================] - 20s 150ms/step - loss: 0.5396 - accuracy: 0.8046 - val_loss: 0.7684 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.71154\n",
            "Epoch 14/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.5601 - accuracy: 0.7996 - val_loss: 0.6901 - val_accuracy: 0.7436\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.71154 to 0.74359, saving model to model.h5\n",
            "Epoch 15/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.5644 - accuracy: 0.7870 - val_loss: 0.6856 - val_accuracy: 0.7286\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.74359\n",
            "Epoch 16/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.5648 - accuracy: 0.7972 - val_loss: 0.9164 - val_accuracy: 0.7051\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.74359\n",
            "Epoch 17/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.5882 - accuracy: 0.7755 - val_loss: 0.7807 - val_accuracy: 0.7094\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.74359\n",
            "Epoch 18/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.5292 - accuracy: 0.8103 - val_loss: 0.6106 - val_accuracy: 0.7756\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.74359 to 0.77564, saving model to model.h5\n",
            "Epoch 19/120\n",
            "132/132 [==============================] - 20s 150ms/step - loss: 0.5164 - accuracy: 0.8174 - val_loss: 0.6017 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.77564\n",
            "Epoch 20/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.5192 - accuracy: 0.8102 - val_loss: 0.7444 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.77564\n",
            "Epoch 21/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.5415 - accuracy: 0.7989 - val_loss: 0.7159 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.77564\n",
            "Epoch 22/120\n",
            "132/132 [==============================] - 20s 150ms/step - loss: 0.5634 - accuracy: 0.7983 - val_loss: 0.6278 - val_accuracy: 0.7735\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.77564\n",
            "Epoch 23/120\n",
            "132/132 [==============================] - 20s 150ms/step - loss: 0.4974 - accuracy: 0.8191 - val_loss: 0.6158 - val_accuracy: 0.7564\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.77564\n",
            "Epoch 24/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.5276 - accuracy: 0.7988 - val_loss: 0.5634 - val_accuracy: 0.8056\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.77564 to 0.80556, saving model to model.h5\n",
            "Epoch 25/120\n",
            "132/132 [==============================] - 20s 150ms/step - loss: 0.5208 - accuracy: 0.8087 - val_loss: 0.5802 - val_accuracy: 0.7842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.80556\n",
            "Epoch 26/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.4921 - accuracy: 0.8204 - val_loss: 0.5859 - val_accuracy: 0.7906\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.80556\n",
            "Epoch 27/120\n",
            "132/132 [==============================] - 20s 150ms/step - loss: 0.5397 - accuracy: 0.8015 - val_loss: 0.6290 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.80556\n",
            "Epoch 28/120\n",
            "132/132 [==============================] - 20s 150ms/step - loss: 0.5150 - accuracy: 0.8058 - val_loss: 0.6132 - val_accuracy: 0.7628\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.80556\n",
            "Epoch 29/120\n",
            "132/132 [==============================] - 20s 150ms/step - loss: 0.5116 - accuracy: 0.8044 - val_loss: 0.6028 - val_accuracy: 0.7756\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.80556\n",
            "Epoch 30/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.4951 - accuracy: 0.8163 - val_loss: 0.6976 - val_accuracy: 0.7201\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.80556\n",
            "Epoch 31/120\n",
            "132/132 [==============================] - 20s 150ms/step - loss: 0.4590 - accuracy: 0.8369 - val_loss: 0.5881 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.80556\n",
            "Epoch 32/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4763 - accuracy: 0.8265 - val_loss: 0.5820 - val_accuracy: 0.7885\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.80556\n",
            "Epoch 33/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.4983 - accuracy: 0.8134 - val_loss: 0.5771 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.80556\n",
            "Epoch 34/120\n",
            "132/132 [==============================] - 20s 150ms/step - loss: 0.4938 - accuracy: 0.8164 - val_loss: 0.6834 - val_accuracy: 0.7415\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.80556\n",
            "Epoch 35/120\n",
            "132/132 [==============================] - 20s 150ms/step - loss: 0.4905 - accuracy: 0.8168 - val_loss: 0.5880 - val_accuracy: 0.7778\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.80556\n",
            "Epoch 36/120\n",
            "132/132 [==============================] - 20s 150ms/step - loss: 0.4851 - accuracy: 0.8093 - val_loss: 0.5633 - val_accuracy: 0.7778\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.80556\n",
            "Epoch 37/120\n",
            "132/132 [==============================] - 20s 150ms/step - loss: 0.4946 - accuracy: 0.8185 - val_loss: 0.5810 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.80556\n",
            "Epoch 38/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.4877 - accuracy: 0.8142 - val_loss: 0.5991 - val_accuracy: 0.7735\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.80556\n",
            "Epoch 39/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4960 - accuracy: 0.8094 - val_loss: 0.5899 - val_accuracy: 0.7735\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.80556\n",
            "Epoch 40/120\n",
            "132/132 [==============================] - 21s 156ms/step - loss: 0.4622 - accuracy: 0.8254 - val_loss: 0.5910 - val_accuracy: 0.7799\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.80556\n",
            "Epoch 41/120\n",
            "132/132 [==============================] - 20s 154ms/step - loss: 0.4819 - accuracy: 0.8260 - val_loss: 0.5611 - val_accuracy: 0.7842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.80556\n",
            "Epoch 42/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4586 - accuracy: 0.8263 - val_loss: 0.5458 - val_accuracy: 0.7949\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.80556\n",
            "Epoch 43/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4645 - accuracy: 0.8273 - val_loss: 0.5205 - val_accuracy: 0.8034\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.80556\n",
            "Epoch 44/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4414 - accuracy: 0.8328 - val_loss: 0.5235 - val_accuracy: 0.7991\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.80556\n",
            "Epoch 45/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4285 - accuracy: 0.8456 - val_loss: 0.5147 - val_accuracy: 0.8056\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.80556\n",
            "Epoch 46/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.4423 - accuracy: 0.8390 - val_loss: 0.5112 - val_accuracy: 0.8141\n",
            "\n",
            "Epoch 00046: val_accuracy improved from 0.80556 to 0.81410, saving model to model.h5\n",
            "Epoch 47/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4198 - accuracy: 0.8432 - val_loss: 0.5025 - val_accuracy: 0.8077\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.81410\n",
            "Epoch 48/120\n",
            "132/132 [==============================] - 20s 153ms/step - loss: 0.4082 - accuracy: 0.8572 - val_loss: 0.5154 - val_accuracy: 0.7970\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.81410\n",
            "Epoch 49/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4152 - accuracy: 0.8551 - val_loss: 0.5069 - val_accuracy: 0.8098\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.81410\n",
            "Epoch 50/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4107 - accuracy: 0.8484 - val_loss: 0.5200 - val_accuracy: 0.8056\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.81410\n",
            "Epoch 51/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4107 - accuracy: 0.8495 - val_loss: 0.5311 - val_accuracy: 0.7991\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.81410\n",
            "Epoch 52/120\n",
            "132/132 [==============================] - 20s 153ms/step - loss: 0.4320 - accuracy: 0.8391 - val_loss: 0.5497 - val_accuracy: 0.7885\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.81410\n",
            "Epoch 53/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.4293 - accuracy: 0.8386 - val_loss: 0.5382 - val_accuracy: 0.7885\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.81410\n",
            "Epoch 54/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.4076 - accuracy: 0.8503 - val_loss: 0.5025 - val_accuracy: 0.8034\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.81410\n",
            "Epoch 55/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4218 - accuracy: 0.8399 - val_loss: 0.5232 - val_accuracy: 0.7991\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.81410\n",
            "Epoch 56/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4066 - accuracy: 0.8492 - val_loss: 0.5284 - val_accuracy: 0.8056\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.81410\n",
            "Epoch 57/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4047 - accuracy: 0.8520 - val_loss: 0.5066 - val_accuracy: 0.8098\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.81410\n",
            "Epoch 58/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.4036 - accuracy: 0.8528 - val_loss: 0.5230 - val_accuracy: 0.8034\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.81410\n",
            "Epoch 59/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.4005 - accuracy: 0.8483 - val_loss: 0.5407 - val_accuracy: 0.7949\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.81410\n",
            "Epoch 60/120\n",
            "132/132 [==============================] - 20s 153ms/step - loss: 0.4004 - accuracy: 0.8508 - val_loss: 0.5231 - val_accuracy: 0.7970\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.81410\n",
            "Epoch 61/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.3748 - accuracy: 0.8695 - val_loss: 0.5511 - val_accuracy: 0.7949\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.81410\n",
            "Epoch 62/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.4004 - accuracy: 0.8579 - val_loss: 0.5287 - val_accuracy: 0.7885\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.81410\n",
            "Epoch 63/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4028 - accuracy: 0.8469 - val_loss: 0.5353 - val_accuracy: 0.8034\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.81410\n",
            "Epoch 64/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4101 - accuracy: 0.8519 - val_loss: 0.5397 - val_accuracy: 0.7970\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.81410\n",
            "Epoch 65/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3869 - accuracy: 0.8640 - val_loss: 0.5293 - val_accuracy: 0.8013\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.81410\n",
            "Epoch 66/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.3979 - accuracy: 0.8495 - val_loss: 0.5318 - val_accuracy: 0.8098\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.81410\n",
            "Epoch 67/120\n",
            "132/132 [==============================] - 20s 153ms/step - loss: 0.3917 - accuracy: 0.8518 - val_loss: 0.5207 - val_accuracy: 0.8013\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.81410\n",
            "Epoch 68/120\n",
            "132/132 [==============================] - 20s 154ms/step - loss: 0.3881 - accuracy: 0.8585 - val_loss: 0.5171 - val_accuracy: 0.8056\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.81410\n",
            "Epoch 69/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.3935 - accuracy: 0.8602 - val_loss: 0.5175 - val_accuracy: 0.7970\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.81410\n",
            "Epoch 70/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3913 - accuracy: 0.8518 - val_loss: 0.5009 - val_accuracy: 0.8226\n",
            "\n",
            "Epoch 00070: val_accuracy improved from 0.81410 to 0.82265, saving model to model.h5\n",
            "Epoch 71/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3944 - accuracy: 0.8599 - val_loss: 0.5221 - val_accuracy: 0.7949\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.82265\n",
            "Epoch 72/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4011 - accuracy: 0.8491 - val_loss: 0.5273 - val_accuracy: 0.7991\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.82265\n",
            "Epoch 73/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.4016 - accuracy: 0.8522 - val_loss: 0.5196 - val_accuracy: 0.8120\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.82265\n",
            "Epoch 74/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.4115 - accuracy: 0.8453 - val_loss: 0.5383 - val_accuracy: 0.8013\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.82265\n",
            "Epoch 75/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.3927 - accuracy: 0.8467 - val_loss: 0.5292 - val_accuracy: 0.8013\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.82265\n",
            "Epoch 76/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.4090 - accuracy: 0.8399 - val_loss: 0.5211 - val_accuracy: 0.7906\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.82265\n",
            "Epoch 77/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.3933 - accuracy: 0.8512 - val_loss: 0.5461 - val_accuracy: 0.8034\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.82265\n",
            "Epoch 78/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3999 - accuracy: 0.8532 - val_loss: 0.5399 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.82265\n",
            "Epoch 79/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.3961 - accuracy: 0.8608 - val_loss: 0.5237 - val_accuracy: 0.8056\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.82265\n",
            "Epoch 80/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4058 - accuracy: 0.8619 - val_loss: 0.5379 - val_accuracy: 0.7991\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.82265\n",
            "Epoch 81/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3825 - accuracy: 0.8590 - val_loss: 0.5291 - val_accuracy: 0.7991\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.82265\n",
            "Epoch 82/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.4023 - accuracy: 0.8528 - val_loss: 0.5497 - val_accuracy: 0.8077\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.82265\n",
            "Epoch 83/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4068 - accuracy: 0.8458 - val_loss: 0.5276 - val_accuracy: 0.7906\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.82265\n",
            "Epoch 84/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3838 - accuracy: 0.8547 - val_loss: 0.5080 - val_accuracy: 0.8034\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.82265\n",
            "Epoch 85/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.3839 - accuracy: 0.8574 - val_loss: 0.5195 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.82265\n",
            "Epoch 86/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.4164 - accuracy: 0.8372 - val_loss: 0.5305 - val_accuracy: 0.8077\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.82265\n",
            "Epoch 87/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.3966 - accuracy: 0.8399 - val_loss: 0.5422 - val_accuracy: 0.8120\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.82265\n",
            "Epoch 88/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3782 - accuracy: 0.8533 - val_loss: 0.5242 - val_accuracy: 0.8077\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.82265\n",
            "Epoch 89/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.3885 - accuracy: 0.8615 - val_loss: 0.5071 - val_accuracy: 0.8034\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.82265\n",
            "Epoch 90/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.3980 - accuracy: 0.8504 - val_loss: 0.5155 - val_accuracy: 0.8056\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.82265\n",
            "Epoch 91/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3980 - accuracy: 0.8546 - val_loss: 0.5292 - val_accuracy: 0.7991\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.82265\n",
            "Epoch 92/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.3719 - accuracy: 0.8613 - val_loss: 0.5475 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.82265\n",
            "Epoch 93/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3866 - accuracy: 0.8524 - val_loss: 0.5056 - val_accuracy: 0.8184\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.82265\n",
            "Epoch 94/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3987 - accuracy: 0.8471 - val_loss: 0.5361 - val_accuracy: 0.7991\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.82265\n",
            "Epoch 95/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3826 - accuracy: 0.8602 - val_loss: 0.5288 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.82265\n",
            "Epoch 96/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3939 - accuracy: 0.8501 - val_loss: 0.5245 - val_accuracy: 0.8098\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.82265\n",
            "Epoch 97/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.3991 - accuracy: 0.8533 - val_loss: 0.5105 - val_accuracy: 0.8013\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.82265\n",
            "Epoch 98/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.3652 - accuracy: 0.8676 - val_loss: 0.5522 - val_accuracy: 0.8013\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.82265\n",
            "Epoch 99/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.3828 - accuracy: 0.8564 - val_loss: 0.5322 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.82265\n",
            "Epoch 100/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3945 - accuracy: 0.8506 - val_loss: 0.5064 - val_accuracy: 0.8205\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.82265\n",
            "Epoch 101/120\n",
            "132/132 [==============================] - 20s 153ms/step - loss: 0.3884 - accuracy: 0.8535 - val_loss: 0.5002 - val_accuracy: 0.8056\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.82265\n",
            "Epoch 102/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3873 - accuracy: 0.8579 - val_loss: 0.5325 - val_accuracy: 0.8034\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.82265\n",
            "Epoch 103/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.3920 - accuracy: 0.8603 - val_loss: 0.5178 - val_accuracy: 0.8013\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.82265\n",
            "Epoch 104/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3805 - accuracy: 0.8529 - val_loss: 0.5150 - val_accuracy: 0.8077\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.82265\n",
            "Epoch 105/120\n",
            "132/132 [==============================] - 20s 153ms/step - loss: 0.3895 - accuracy: 0.8594 - val_loss: 0.5248 - val_accuracy: 0.8162\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.82265\n",
            "Epoch 106/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3874 - accuracy: 0.8620 - val_loss: 0.5288 - val_accuracy: 0.8034\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.82265\n",
            "Epoch 107/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4058 - accuracy: 0.8487 - val_loss: 0.5465 - val_accuracy: 0.8056\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.82265\n",
            "Epoch 108/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3899 - accuracy: 0.8538 - val_loss: 0.5044 - val_accuracy: 0.7949\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.82265\n",
            "Epoch 109/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3906 - accuracy: 0.8636 - val_loss: 0.5515 - val_accuracy: 0.8098\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.82265\n",
            "Epoch 110/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.4080 - accuracy: 0.8545 - val_loss: 0.5436 - val_accuracy: 0.7949\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.82265\n",
            "Epoch 111/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3829 - accuracy: 0.8592 - val_loss: 0.5116 - val_accuracy: 0.8077\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.82265\n",
            "Epoch 112/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3982 - accuracy: 0.8506 - val_loss: 0.5275 - val_accuracy: 0.8034\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.82265\n",
            "Epoch 113/120\n",
            "132/132 [==============================] - 20s 153ms/step - loss: 0.4092 - accuracy: 0.8454 - val_loss: 0.5304 - val_accuracy: 0.8141\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.82265\n",
            "Epoch 114/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3832 - accuracy: 0.8540 - val_loss: 0.5168 - val_accuracy: 0.7991\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.82265\n",
            "Epoch 115/120\n",
            "132/132 [==============================] - 20s 151ms/step - loss: 0.4029 - accuracy: 0.8478 - val_loss: 0.5196 - val_accuracy: 0.8056\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.82265\n",
            "Epoch 116/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3826 - accuracy: 0.8598 - val_loss: 0.5080 - val_accuracy: 0.8120\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.82265\n",
            "Epoch 117/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3965 - accuracy: 0.8534 - val_loss: 0.5095 - val_accuracy: 0.8141\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.82265\n",
            "Epoch 118/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3749 - accuracy: 0.8589 - val_loss: 0.5470 - val_accuracy: 0.8120\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.82265\n",
            "Epoch 119/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3826 - accuracy: 0.8639 - val_loss: 0.5136 - val_accuracy: 0.8120\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.82265\n",
            "Epoch 120/120\n",
            "132/132 [==============================] - 20s 152ms/step - loss: 0.3877 - accuracy: 0.8580 - val_loss: 0.5138 - val_accuracy: 0.8098\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.82265\n"
          ]
        }
      ],
      "source": [
        "# We use 120 epoch and batch size equal to 32.\n",
        "max_epochs = 120\n",
        "batch_size = 32\n",
        "scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "#Checkpoint cal_accuracy to see our best accuracy score in val.\n",
        "checkpoint = ModelCheckpoint('model.h5', \n",
        "                             verbose=1, \n",
        "                             monitor='val_accuracy', \n",
        "                             save_best_only=True)\n",
        "#Applying the model in our dataset.\n",
        "history = model.fit(datagen.flow(x=X_train, y=Y_train),\n",
        "                    validation_data=datagen.flow(X_val,Y_val),\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=max_epochs,\n",
        "                    callbacks = [scheduler, checkpoint],\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "2uMkHhiGScO2",
        "outputId": "3b81f079-051b-475f-e801-d23ac59fecfd"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Creating plot with Train set accuracy, and Validation set accuracy checking about overfitting\n",
        "fig, ax = plt.subplots(figsize=(10,8))\n",
        "fig.set_facecolor('lightgray')\n",
        "plt.plot(history.history['accuracy'], 'b')\n",
        "plt.plot(history.history['val_accuracy'], 'r')\n",
        "plt.legend(['train acc', 'val acc'], fontsize=16)\n",
        "plt.xlabel('Epochs')\n",
        "plt.grid(b=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ5TKPJE7FUS"
      },
      "source": [
        "## **Making Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9MDnL_0P9ys",
        "outputId": "2a13cb7f-b9f7-4494-9a66-2930a8c603ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[9.9579382e-01 9.8306010e-04 3.2231319e-03]\n",
            " [3.4007359e-02 7.2296846e-01 2.4302414e-01]\n",
            " [7.7447523e-03 1.1290122e-01 8.7935400e-01]\n",
            " ...\n",
            " [1.9368990e-05 2.8099859e-01 7.1898204e-01]\n",
            " [3.4785924e-05 8.2143039e-01 1.7853491e-01]\n",
            " [6.2132413e-03 6.0920238e-01 3.8458446e-01]]\n"
          ]
        }
      ],
      "source": [
        "#Apply our model to our test dataset\n",
        "Y_test_prob = model.predict(X_test)\n",
        "print(Y_test_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qeu-YMyU2sl",
        "outputId": "a121f344-65b8-4195-9442-01ae8e6832fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 2 ... 2 1 1]\n"
          ]
        }
      ],
      "source": [
        "#Modify to 0,1,2\n",
        "Y_test = Y_test_prob.argmax(axis=-1)\n",
        "print(Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "sarUpaaiU5pc",
        "outputId": "23f7a295-5982-4665-949b-64ebb20df273"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>class_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>img_4358977458434011046.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>img_5224016757187192130.jpg</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>img_3065202206106254707.jpg</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>img_6304894865561547174.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>img_3371338542810939877.jpg</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>img_946797649386887230.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>img_3282952127033205295.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>img_3090153597350897926.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>img_4875755942606271156.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>img_4787814488043878610.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     file_name  class_id\n",
              "0  img_4358977458434011046.jpg         1\n",
              "1  img_5224016757187192130.jpg         2\n",
              "2  img_3065202206106254707.jpg         2\n",
              "3  img_6304894865561547174.jpg         1\n",
              "4  img_3371338542810939877.jpg         2\n",
              "5   img_946797649386887230.jpg         0\n",
              "6  img_3282952127033205295.jpg         0\n",
              "7  img_3090153597350897926.jpg         1\n",
              "8  img_4875755942606271156.jpg         1\n",
              "9  img_4787814488043878610.jpg         1"
            ]
          },
          "execution_count": 29,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Create dataframe with file names and labels for test dataset\n",
        "test_names = [i for i in os.listdir(test_path)]\n",
        "\n",
        "df_test = pd.DataFrame (test_names, columns=['file_name'])\n",
        "\n",
        "df_test.head()\n",
        "df_test['class_id'] = Y_test\n",
        "df_labels.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMbES7oubjKa"
      },
      "outputs": [],
      "source": [
        "#Saving as csv to our Google Drive.\n",
        "df_test.to_csv('/content/drive/MyDrive/Georgios Papageorgiou subm 256b.csv', sep = ',', index = False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "img size 200.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
